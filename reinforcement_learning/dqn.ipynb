{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd20a59-82c4-41b7-9863-b3b5f5779c85",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fde6e-6067-4ebc-aaa4-3e765286634c",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86df9de5-69aa-499a-b826-e30aee7eab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "from IPython import display\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# trick to import from relative path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from replay_memory import ReplayMemory, Transition\n",
    "from nb_utils.widgets import ArrayRenderWidget\n",
    "\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471523f8-4c2c-45a5-9b55-fc5e90890a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed22b710-23e4-4537-8f88-11b41b10679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmax-schik\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f354bd4d-8a57-472a-869f-31ec96e60827",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_episodes\": 2000,\n",
    "    \"batch_size\": 128,\n",
    "    \"gamma\": 0.99,\n",
    "    \"eps_start\": 0.9,\n",
    "    \"eps_end\": 0.05,\n",
    "    \"eps_decay\": 1000,\n",
    "    \"tau\": 0.005,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env\": \"Acrobot-v1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a53676-2f4d-4627-a6ba-b9e564c5b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(config[\"env\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e314c7-3305-4752-9bf3-c8cf05d3a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "933db978-08a0-41fb-b487-65a0d1e4704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, target_net, policy_net, memory, update_interval=1):\n",
    "        self.env = env\n",
    "        self.target_net = target_net\n",
    "        self.policy_net = policy_net\n",
    "        self.memory = memory\n",
    "        self.update_interval = update_interval\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=config[\"lr\"], amsgrad=True)\n",
    "        \n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        self.t_step = (self.t_step + 1) % self.update_interval\n",
    "\n",
    "        if self.t_step == 0 and len(self.memory) > config[\"batch_size\"]:\n",
    "            batch = self.memory.sample(config[\"batch_size\"])\n",
    "            return self.learn(batch)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        sample = random.random()\n",
    "        if sample > eps:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], device=device, dtype=torch.long)                \n",
    "\n",
    "    def learn(self, batch):\n",
    "        # This converts batch-array of Transitions to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*batch))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(config[\"batch_size\"], device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = reward_batch + (next_state_values * config[\"gamma\"])\n",
    "    \n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update()\n",
    "    \n",
    "        return loss\n",
    "\n",
    "    def soft_update(self):\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = self.target_net.state_dict()\n",
    "        policy_net_state_dict = self.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*config[\"tau\"] + target_net_state_dict[key]*(1-config[\"tau\"])\n",
    "        self.target_net.load_state_dict(target_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d79a7d5-d7e0-48c3-810d-925945479158",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "agent = DQNAgent(env, target_net, policy_net, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6854043-4239-4876-8389-937809082796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ws/reinforcement_learning/wandb/run-20240421_191426-cuqqg0g4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/max-schik/learn-rl.dqn.Acrobot-v1/runs/cuqqg0g4' target=\"_blank\">stellar-haze-2</a></strong> to <a href='https://wandb.ai/max-schik/learn-rl.dqn.Acrobot-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/max-schik/learn-rl.dqn.Acrobot-v1' target=\"_blank\">https://wandb.ai/max-schik/learn-rl.dqn.Acrobot-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/max-schik/learn-rl.dqn.Acrobot-v1/runs/cuqqg0g4' target=\"_blank\">https://wandb.ai/max-schik/learn-rl.dqn.Acrobot-v1/runs/cuqqg0g4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:39<00:00,  5.01it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>length</td><td>█▁▂▃▁▁█▁▁▁▁▁▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>loss</td><td>▁▄▇▇▇██▇█▇█▇▆▆▆▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▄</td></tr><tr><td>reward</td><td>▁█▇▆██▁█████▇▇▇▇██▇▇████████▆██████▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>length</td><td>75</td></tr><tr><td>loss</td><td>0.26157</td></tr><tr><td>reward</td><td>-74.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-haze-2</strong> at: <a href='https://wandb.ai/max-schik/learn-rl.dqn.Acrobot-v1/runs/cuqqg0g4' target=\"_blank\">https://wandb.ai/max-schik/learn-rl.dqn.Acrobot-v1/runs/cuqqg0g4</a><br/> View project at: <a href='https://wandb.ai/max-schik/learn-rl.dqn.Acrobot-v1' target=\"_blank\">https://wandb.ai/max-schik/learn-rl.dqn.Acrobot-v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240421_191426-cuqqg0g4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=f\"learn-rl.dqn.{config['env']}\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "step = 0\n",
    "\n",
    "for i_episode in tqdm(range(config[\"num_episodes\"])):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    rewards = 0\n",
    "    losses = []\n",
    "    \n",
    "    for t in count():\n",
    "        eps = config[\"eps_end\"] + (config[\"eps_start\"] - config[\"eps_end\"]) * \\\n",
    "              math.exp(-1. * step / config[\"eps_decay\"])\n",
    "        action = agent.act(state, eps)\n",
    "        step += 1\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        loss = agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        rewards += reward\n",
    "        if loss is not None:\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        if done:\n",
    "            wandb.log({\"reward\": rewards, \"loss\": sum(losses) / max(len(losses), 1), \"length\": t+1})\n",
    "            break\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "833a4fe9-5e00-43ca-a598-259f4c2fac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), f\"../weights/dqn_{config['env']}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef503c3c-6f67-45ff-bcbe-31a9585cc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_widget = ArrayRenderWidget(\n",
    "    format='png',\n",
    "    width=600,\n",
    "    height=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8be7d024-3a6c-450e-8ed7-9d3e16040269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85489d81edbc47f890b17d994f6d5321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ArrayRenderWidget(value=b'', height='400', width='600')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(config[\"env\"], render_mode=\"rgb_array\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "display.display(image_widget)\n",
    "image_widget.render(env.render())\n",
    "for _ in range(1000):\n",
    "    image_widget.render(env.render())\n",
    "\n",
    "\n",
    "    # action = env.action_space.sample()\n",
    "    rewards = policy_net(torch.tensor(observation).to(device))\n",
    "    action = torch.argmax(rewards)\n",
    "    action = action.detach().cpu().numpy()\n",
    "    \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(1/30)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
