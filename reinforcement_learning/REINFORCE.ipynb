{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b10dcbf-1c24-4b59-a695-d72c41568468",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41e1f40-055d-4198-95da-993bb0a3590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "from IPython import display\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# trick to import from relative path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from nb_utils.widgets import ArrayRenderWidget\n",
    "\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ef1e2c-ad28-4ae4-9226-7ae2f9859413",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a1ae1c-7f29-467b-adb8-162b4b7c26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5af50c62-f45f-4385-a606-f084cdd6eda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmax-schik\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56de2494-6248-43b8-a517-d17de5f0af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_episodes\": 10000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env\": \"Pendulum-v1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad91d7fe-da13-4f5a-8368-fd8706ecf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(config[\"env\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1f9856d-263e-4484-9313-49d94e9d91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        \n",
    "        self.mean = nn.Linear(128, n_actions)\n",
    "        self.stddev = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.layer1(x))\n",
    "        x = F.tanh(self.layer2(x))\n",
    "        mean = self.mean(x)\n",
    "        stddev = torch.log(1 + torch.exp(self.stddev(x)))\n",
    "        return mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceeb9964-5c60-42b6-9810-af5a63a05c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = PolicyNetwork(6, 6)\n",
    "x = torch.zeros((128, 6, ))\n",
    "y = net(x)\n",
    "\n",
    "dist = Normal(*y)\n",
    "\n",
    "dist.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e7ac3c-f405-412a-9598-820ff97b94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-6\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, policy_net, update_interval=1):\n",
    "        self.env = env\n",
    "        self.policy_net = policy_net\n",
    "        self.update_interval = update_interval\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=config[\"lr\"], amsgrad=True)\n",
    "        \n",
    "        self.t_step = 0\n",
    "\n",
    "    def act(self, state, deterministic=False):\n",
    "        mean, stddev = self.policy_net(state)\n",
    "        dist = Normal(mean + EPS, stddev + EPS)\n",
    "        if deterministic:\n",
    "            action = mean\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.detach().cpu().numpy(), log_prob\n",
    "        \n",
    "    def learn(self, log_probs, rewards):\n",
    "        running_g = 0\n",
    "        gs = []\n",
    "\n",
    "        for R in rewards[::-1]:\n",
    "            running_g = R + config[\"gamma\"] * running_g\n",
    "            gs.insert(0, running_g)\n",
    "\n",
    "        deltas = torch.tensor(gs)\n",
    "\n",
    "        loss = 0\n",
    "        for log_prob, delta in zip(log_probs, deltas):\n",
    "            loss += log_prob.mean() * delta * -1\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fae6522-3ae3-4c8b-8f6d-753d0a4e0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = PolicyNetwork(n_observations, n_actions).to(device)\n",
    "\n",
    "agent = Agent(env, policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fdb4a9e-a103-4a4d-8a96-ee15a9a2c543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.84871566, -0.5288495 ,  0.47445923], dtype=float32),\n",
       " -0.40965866722013544,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(agent.act(torch.tensor(env.observation_space.sample(), device=device))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e308f9e-1619-4251-9011-5e928c92ee21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ws/reinforcement_learning/wandb/run-20240421_233819-mkml3t9f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/max-schik/learn-rl.REINFORCE.Pendulum-v1/runs/mkml3t9f' target=\"_blank\">deft-oath-13</a></strong> to <a href='https://wandb.ai/max-schik/learn-rl.REINFORCE.Pendulum-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/max-schik/learn-rl.REINFORCE.Pendulum-v1' target=\"_blank\">https://wandb.ai/max-schik/learn-rl.REINFORCE.Pendulum-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/max-schik/learn-rl.REINFORCE.Pendulum-v1/runs/mkml3t9f' target=\"_blank\">https://wandb.ai/max-schik/learn-rl.REINFORCE.Pendulum-v1/runs/mkml3t9f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [39:06<00:00,  4.26it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▆▅▅▄▄▅▆▆▃▃▅▄▃▄▃▅▂▇▆▃▃▄▄▅▄▅▇█▁█▁█▆▆▆▂▆▂▆▂</td></tr><tr><td>reward</td><td>█▆▇▄▅▂▃▂▁▂▁▂▁▂▂▂▂▂▂▁▂▁▂▂▂▂▂▁▂▂▂▁▂▂▁▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode_length</td><td>200</td></tr><tr><td>loss</td><td>-184772.35083</td></tr><tr><td>reward</td><td>-1525.43204</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deft-oath-13</strong> at: <a href='https://wandb.ai/max-schik/learn-rl.REINFORCE.Pendulum-v1/runs/mkml3t9f' target=\"_blank\">https://wandb.ai/max-schik/learn-rl.REINFORCE.Pendulum-v1/runs/mkml3t9f</a><br/> View project at: <a href='https://wandb.ai/max-schik/learn-rl.REINFORCE.Pendulum-v1' target=\"_blank\">https://wandb.ai/max-schik/learn-rl.REINFORCE.Pendulum-v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240421_233819-mkml3t9f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=f\"learn-rl.REINFORCE.{config['env']}\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "step = 0\n",
    "\n",
    "for i_episode in tqdm(range(config[\"num_episodes\"])):\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    \n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    \n",
    "    for t in count():\n",
    "        action, log_prob = agent.act(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    loss = agent.learn(log_probs, rewards).detach().cpu().numpy()\n",
    "        \n",
    "    wandb.log({\"reward\": sum(rewards), \"loss\": loss, \"episode_length\": t+1})\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34aa81f7-8ddd-4bab-b506-b1a3757a3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), f\"../weights/REINFORCE_{config['env']}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03aa27f3-7d59-4d63-97e5-c026b197311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_widget = ArrayRenderWidget(\n",
    "    format='png',\n",
    "    width=600,\n",
    "    height=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7e99a-8ca0-491e-b77b-405dc40bb594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a862ddf8ae3c4e059f42bc5abc728a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ArrayRenderWidget(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01\\xf4\\x00\\x00\\x01\\xf4\\x08\\x02\\x00\\x00\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(config[\"env\"], render_mode=\"rgb_array\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "display.display(image_widget)\n",
    "image_widget.render(env.render())\n",
    "for _ in range(1000):\n",
    "    image_widget.render(env.render())\n",
    "\n",
    "\n",
    "    # action = env.action_space.sample()\n",
    "    action = agent.act(torch.tensor(observation, device=device), True)[0]\n",
    "    \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(1/30)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d6157a-a0fa-4c8e-a4d4-77218b42cd9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
